{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fe80c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabData/followers.txt\n",
      "LabData/.DS_Store\n",
      "LabData/notebook.log\n",
      "LabData/nycweather.csv\n",
      "LabData/pom.xml\n",
      "LabData/README.md\n",
      "LabData/nyctaxi100.csv\n",
      "LabData/users.txt\n",
      "LabData/nyctaxisub.csv\n",
      "LabData/taxistreams.py\n",
      "LabData/nyctaxi.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "# actual_path = '/Users/danishkarur/opt/Python Scripts/Learnings_CognitiveClasses/Spark/LabData'\n",
    "for path in Path('LabData').iterdir():\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d51ebdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/20 20:45:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94921568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dc77bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme = sc.textFile('LabData/README.md')\n",
    "readme.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fa4be6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Apache Spark'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e3d34d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "linesWithSpark = readme.filter(lambda line:'Spark' in line)\n",
    "print(linesWithSpark.count())\n",
    "# readme.filter(lambda line: \"Spark\" in line).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92733f7",
   "metadata": {},
   "source": [
    "More on RDD Operations\n",
    "\n",
    "This section builds upon the previous section. In this section, you will see that RDD can be used for more complex computations. You will find the line from that \"README.md\" file with the most words in it.\n",
    "\n",
    "Run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d687569f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.map(lambda line:len(line.split())).reduce(lambda a,b: a if (a>b) else b)\n",
    "\n",
    "# There are two parts to this. The first maps a line to an integer value, \n",
    "# the number of words in that line. \n",
    "# In the second part reduce is called to find the line with the most words in it. \n",
    "# The arguments to map and reduce are Python anonymous functions (lambdas), \n",
    "# but you can use any top level Python functions. In the next step, \n",
    "# you’ll define a max function to illustrate this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e703357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define the max_ function\n",
    "\n",
    "def my_max(a, b):\n",
    "    if a > b:\n",
    "        return a\n",
    "    else:\n",
    "        return b\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afa3c22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.map(lambda line:len(line.split())).reduce(my_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a38192bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordsCount = readme.flatMap(lambda line:line.split()).map(lambda word:(word,1)).reduceByKey(lambda a,b: a+b)\n",
    "WordsCount.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dfcb2d",
   "metadata": {},
   "source": [
    "Here we combined the flatMap, map, and the reduceByKey functions to do a word count of each word in the readme file.\n",
    "\n",
    "To collect the word counts, use the collect action.\n",
    "\n",
    "It should be noted that the collect function brings all of the data into the driver node. For a small dataset, \n",
    "this is acceptable but, for a large dataset this can cause an Out Of Memory error. \n",
    "It is recommended to use collect() for testing only. The safer approach is to use the take() function e.g. print take(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c6971c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 1),\n",
       " ('Apache', 1),\n",
       " ('Spark', 14),\n",
       " ('is', 6),\n",
       " ('It', 2),\n",
       " ('provides', 1),\n",
       " ('high-level', 1),\n",
       " ('APIs', 1),\n",
       " ('in', 5),\n",
       " ('Scala,', 1),\n",
       " ('Java,', 1),\n",
       " ('an', 3),\n",
       " ('optimized', 1),\n",
       " ('engine', 1),\n",
       " ('supports', 2),\n",
       " ('computation', 1),\n",
       " ('analysis.', 1),\n",
       " ('set', 2),\n",
       " ('of', 5),\n",
       " ('tools', 1),\n",
       " ('SQL', 2),\n",
       " ('MLlib', 1),\n",
       " ('machine', 1),\n",
       " ('learning,', 1),\n",
       " ('GraphX', 1),\n",
       " ('graph', 1),\n",
       " ('processing,', 1),\n",
       " ('Documentation', 1),\n",
       " ('latest', 1),\n",
       " ('programming', 1),\n",
       " ('guide,', 1),\n",
       " ('[project', 2),\n",
       " ('README', 1),\n",
       " ('only', 1),\n",
       " ('basic', 1),\n",
       " ('instructions.', 1),\n",
       " ('Building', 1),\n",
       " ('using', 2),\n",
       " ('[Apache', 1),\n",
       " ('run:', 1),\n",
       " ('do', 2),\n",
       " ('this', 1),\n",
       " ('downloaded', 1),\n",
       " ('documentation', 3),\n",
       " ('project', 1),\n",
       " ('site,', 1),\n",
       " ('at', 2),\n",
       " ('Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1),\n",
       " ('Interactive', 2),\n",
       " ('Shell', 2),\n",
       " ('The', 1),\n",
       " ('way', 1),\n",
       " ('start', 1),\n",
       " ('Try', 1),\n",
       " ('following', 2),\n",
       " ('1000:', 2),\n",
       " ('scala>', 1),\n",
       " ('1000).count()', 1),\n",
       " ('Python', 2),\n",
       " ('Alternatively,', 1),\n",
       " ('use', 3),\n",
       " ('And', 1),\n",
       " ('run', 7),\n",
       " ('Example', 1),\n",
       " ('several', 1),\n",
       " ('programs', 2),\n",
       " ('them,', 1),\n",
       " ('`./bin/run-example', 1),\n",
       " ('[params]`.', 1),\n",
       " ('example:', 1),\n",
       " ('./bin/run-example', 2),\n",
       " ('SparkPi', 2),\n",
       " ('variable', 1),\n",
       " ('when', 1),\n",
       " ('examples', 2),\n",
       " ('spark://', 1),\n",
       " ('URL,', 1),\n",
       " ('YARN,', 1),\n",
       " ('\"local\"', 1),\n",
       " ('locally', 2),\n",
       " ('N', 1),\n",
       " ('abbreviated', 1),\n",
       " ('class', 2),\n",
       " ('name', 1),\n",
       " ('package.', 1),\n",
       " ('instance:', 1),\n",
       " ('print', 1),\n",
       " ('usage', 1),\n",
       " ('help', 1),\n",
       " ('no', 1),\n",
       " ('params', 1),\n",
       " ('are', 1),\n",
       " ('Testing', 1),\n",
       " ('Spark](#building-spark).', 1),\n",
       " ('Once', 1),\n",
       " ('built,', 1),\n",
       " ('tests', 2),\n",
       " ('using:', 1),\n",
       " ('./dev/run-tests', 1),\n",
       " ('Please', 3),\n",
       " ('guidance', 3),\n",
       " ('module,', 1),\n",
       " ('individual', 1),\n",
       " ('Note', 1),\n",
       " ('About', 1),\n",
       " ('uses', 1),\n",
       " ('library', 1),\n",
       " ('HDFS', 1),\n",
       " ('other', 1),\n",
       " ('Hadoop-supported', 1),\n",
       " ('storage', 1),\n",
       " ('systems.', 1),\n",
       " ('Because', 1),\n",
       " ('have', 1),\n",
       " ('changed', 1),\n",
       " ('different', 1),\n",
       " ('versions', 1),\n",
       " ('Hadoop,', 2),\n",
       " ('must', 1),\n",
       " ('against', 1),\n",
       " ('version', 1),\n",
       " ('refer', 2),\n",
       " ('particular', 3),\n",
       " ('distribution', 1),\n",
       " ('Hive', 2),\n",
       " ('Thriftserver', 1),\n",
       " ('distributions.', 1),\n",
       " ('[\"Third', 1),\n",
       " ('distribution.', 1),\n",
       " ('[Configuration', 1),\n",
       " ('Guide](http://spark.apache.org/docs/latest/configuration.html)', 1),\n",
       " ('online', 1),\n",
       " ('overview', 1),\n",
       " ('configure', 1),\n",
       " ('Spark.', 1),\n",
       " ('a', 10),\n",
       " ('fast', 1),\n",
       " ('and', 10),\n",
       " ('general', 2),\n",
       " ('cluster', 2),\n",
       " ('computing', 1),\n",
       " ('system', 1),\n",
       " ('for', 12),\n",
       " ('Big', 1),\n",
       " ('Data.', 1),\n",
       " ('Python,', 2),\n",
       " ('R,', 1),\n",
       " ('that', 3),\n",
       " ('graphs', 1),\n",
       " ('data', 1),\n",
       " ('also', 5),\n",
       " ('rich', 1),\n",
       " ('higher-level', 1),\n",
       " ('including', 3),\n",
       " ('DataFrames,', 1),\n",
       " ('Streaming', 1),\n",
       " ('stream', 1),\n",
       " ('processing.', 1),\n",
       " ('<http://spark.apache.org/>', 1),\n",
       " ('##', 8),\n",
       " ('Online', 1),\n",
       " ('You', 3),\n",
       " ('can', 6),\n",
       " ('find', 1),\n",
       " ('the', 21),\n",
       " ('documentation,', 1),\n",
       " ('on', 6),\n",
       " ('web', 1),\n",
       " ('page](http://spark.apache.org/documentation.html)', 1),\n",
       " ('wiki](https://cwiki.apache.org/confluence/display/SPARK).', 1),\n",
       " ('This', 2),\n",
       " ('file', 1),\n",
       " ('contains', 1),\n",
       " ('setup', 1),\n",
       " ('built', 1),\n",
       " ('Maven](http://maven.apache.org/).', 1),\n",
       " ('To', 2),\n",
       " ('build', 3),\n",
       " ('its', 1),\n",
       " ('example', 3),\n",
       " ('programs,', 1),\n",
       " ('build/mvn', 1),\n",
       " ('-DskipTests', 1),\n",
       " ('clean', 1),\n",
       " ('package', 1),\n",
       " ('(You', 1),\n",
       " ('not', 1),\n",
       " ('need', 1),\n",
       " ('to', 14),\n",
       " ('if', 4),\n",
       " ('you', 4),\n",
       " ('pre-built', 1),\n",
       " ('package.)', 1),\n",
       " ('More', 1),\n",
       " ('detailed', 2),\n",
       " ('available', 1),\n",
       " ('from', 1),\n",
       " ('[\"Building', 1),\n",
       " ('Scala', 2),\n",
       " ('easiest', 1),\n",
       " ('through', 1),\n",
       " ('shell:', 2),\n",
       " ('./bin/spark-shell', 1),\n",
       " ('command,', 2),\n",
       " ('which', 2),\n",
       " ('should', 2),\n",
       " ('return', 2),\n",
       " ('sc.parallelize(1', 1),\n",
       " ('prefer', 1),\n",
       " ('./bin/pyspark', 1),\n",
       " ('>>>', 1),\n",
       " ('sc.parallelize(range(1000)).count()', 1),\n",
       " ('Programs', 1),\n",
       " ('comes', 1),\n",
       " ('with', 4),\n",
       " ('sample', 1),\n",
       " ('`examples`', 2),\n",
       " ('directory.', 1),\n",
       " ('one', 2),\n",
       " ('<class>', 1),\n",
       " ('For', 2),\n",
       " ('will', 1),\n",
       " ('Pi', 1),\n",
       " ('locally.', 1),\n",
       " ('MASTER', 1),\n",
       " ('environment', 1),\n",
       " ('running', 1),\n",
       " ('submit', 1),\n",
       " ('cluster.', 1),\n",
       " ('be', 2),\n",
       " ('mesos://', 1),\n",
       " ('or', 3),\n",
       " ('\"yarn\"', 1),\n",
       " ('thread,', 1),\n",
       " ('\"local[N]\"', 1),\n",
       " ('threads.', 1),\n",
       " ('MASTER=spark://host:7077', 1),\n",
       " ('Many', 1),\n",
       " ('given.', 1),\n",
       " ('Running', 1),\n",
       " ('Tests', 1),\n",
       " ('first', 1),\n",
       " ('requires', 1),\n",
       " ('[building', 1),\n",
       " ('see', 1),\n",
       " ('how', 2),\n",
       " ('[run', 1),\n",
       " ('tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).',\n",
       "  1),\n",
       " ('A', 1),\n",
       " ('Hadoop', 4),\n",
       " ('Versions', 1),\n",
       " ('core', 1),\n",
       " ('talk', 1),\n",
       " ('protocols', 1),\n",
       " ('same', 1),\n",
       " ('your', 1),\n",
       " ('runs.', 1),\n",
       " ('[\"Specifying', 1),\n",
       " ('Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)',\n",
       "  1),\n",
       " ('building', 3),\n",
       " ('See', 1),\n",
       " ('Party', 1),\n",
       " ('Distributions\"](http://spark.apache.org/docs/latest/hadoop-third-party-distributions.html)',\n",
       "  1),\n",
       " ('application', 1),\n",
       " ('works', 1),\n",
       " ('Configuration', 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordsCount.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8be52d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the', 21)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most frequent word in this README file\n",
    "\n",
    "WordsCount.reduce(lambda a,b: a if a[1]>b[1] else b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc49742a",
   "metadata": {},
   "source": [
    "Using Spark caching\n",
    "\n",
    "In this short section, you’ll see how Spark caching can be \n",
    "used to pull data sets into a cluster-wide in-memory cache. \n",
    "This is very useful for accessing repeated data, such as querying a small “hot” dataset or \n",
    "when running an iterative algorithm. Both Python and Scala use the same commands.\n",
    "\n",
    "As a simple example, let’s mark our linesWithSpark dataset to be cached and \n",
    "then invoke the first count operation to tell Spark to cache it. \n",
    "Remember that transformation operations such as cache does not get processed until some action like count() is called. \n",
    "Once you run the second count() operation, \n",
    "you should notice a small increase in speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de5e7e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(linesWithSpark.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f0c5491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import Timer\n",
    "def count():\n",
    "    return linesWithSpark.count()\n",
    "t = Timer(lambda: count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebd64371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9611677909999798\n"
     ]
    }
   ],
   "source": [
    "print(t.timeit(number=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c9e103e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7766033330000255\n"
     ]
    }
   ],
   "source": [
    "linesWithSpark.cache()\n",
    "print(t.timeit(number=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83267861",
   "metadata": {},
   "source": [
    "It may seem silly to cache such a small file, \n",
    "but for larger data sets across tens or hundreds of nodes, \n",
    "this would still work. \n",
    "The second linesWithSpark.count() action runs against \n",
    "the cache and would perform significantly better for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff7d9a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
